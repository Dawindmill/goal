{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepared the using data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'smallTwitter.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b8631c8293cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#[{'twitter_content': 'text,text,text,text', 'hashtag_content': '#whatthewaht'},...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcontent_data\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'smallTwitter.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcontent_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'smallTwitter.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "#save the content data into format like following\n",
    "#[{'twitter_content': 'text,text,text,text', 'hashtag_content': '#whatthewaht'},...]\n",
    "content_data =[]\n",
    "with open('smallTwitter.json') as data_file:\n",
    "    for line in data_file:\n",
    "        content_dic = {}\n",
    "        if line[0] == '{':\n",
    "            line = line[:-1]\n",
    "            if line[-1] == ',':\n",
    "                #get the content\n",
    "                content_dic['twitter_content'] =json.loads(line[:-1]).get('json').get('text')\n",
    "                if len(json.loads(line[:-1]).get('json').get('entities').get('hashtags')) !=0:\n",
    "                    content_dic['hashtag_content'] = json.loads(line[:-1]).get('json').get('entities').get('hashtags')[0].get('text')\n",
    "                content_data.append(content_dic)\n",
    "\n",
    "#create the own word set from the twitter content to expand NLTK wordset\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "\n",
    "words_set = []\n",
    "for twitter in content_data:\n",
    "    words_set +=(word_tokenizer.tokenize(twitter['twitter_content']))\n",
    "words_set = list(set(words_set))\n",
    "\n",
    "from string import punctuation  \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "non_words = list(punctuation)\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "#only need the alphabetic word\n",
    "formartted_twitter_words_set = []\n",
    "for word in words_set:\n",
    "    if (word.isalpha() != False) and (word not in non_words) and (word not in stop_words):\n",
    "        formartted_twitter_words_set.append(lemmatizer.lemmatize(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk_words_set = list(set(nltk.corpus.words.words()))\n",
    "#training whole set\n",
    "training_set = formartted_twitter_words_set+nltk_words_set\n",
    "\n",
    "#split the word in hashtags by using the training set\n",
    "#using backward MaxMatch\n",
    "def find_match_word(hash_content, wordlist):\n",
    "    split_words = []\n",
    "    while len(hash_content) !=0:\n",
    "        #return the index of the matched word\n",
    "        word, index = check_match(hash_content,wordlist)\n",
    "        split_words.append(word)\n",
    "        #remove the matched words from the original tokens\n",
    "        hash_content = hash_content[len(hash_content)*(-1):index]\n",
    "    return split_words\n",
    "\n",
    "#use WordNetLemmatizer to lemmatize the word\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "def check_match(hash_content,wordlist):\n",
    "    count = 0\n",
    "    i = 0\n",
    "    j = len(hash_content)*(-1)\n",
    "    temp = ''\n",
    "    while i>=j:\n",
    "        i -= 1\n",
    "        #save the temporary word and wait for max match\n",
    "        if lemmatizer.lemmatize(hash_content[i:]) in wordlist and i!=j:\n",
    "            temp = hash_content[i:]\n",
    "            count = i\n",
    "        #if already rich the max lenght, return current saved temporary word\n",
    "        elif lemmatizer.lemmatize(hash_content[i:]) in wordlist and i==j:\n",
    "            temp = hash_content[i:]\n",
    "            return temp,i\n",
    "        else:\n",
    "            #base case: if reach the maximum length and the word is not in wordlist\n",
    "            #return current temporary word\n",
    "            if len(temp)>0 and i==j:\n",
    "                return temp,count\n",
    "            #return one single letter if theres no match\n",
    "            elif len(temp) == 0 and i == j:\n",
    "                return hash_content[i:],-1\n",
    "            else:\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the hash tag content \n",
    "It takes a bit long. May need map reduce or HPC to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split the hashtag content now!!\n",
    "for twitter in content_data:\n",
    "    if 'hashtag_content' in twitter.keys():\n",
    "        twitter['hashtag_content'] = find_match_word(twitter['hashtag_content'].lower(),training_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA topic detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from string import punctuation  \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "non_words = list(punctuation)\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "\n",
    "\n",
    "def process_content(tweeter_content):\n",
    "    #only need the alphabetic word\n",
    "    formartted_twitter_content = []\n",
    "    tweeter_content =word_tokenizer.tokenize(tweeter_content)\n",
    "    for word in tweeter_content:\n",
    "        if (word.isalpha() != False) and (word not in non_words) and (word not in stop_words):\n",
    "            formartted_twitter_content.append(lemmatizer.lemmatize(word))\n",
    "    return formartted_twitter_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "for twitter in content_data:\n",
    "    if 'hashtag_content' in twitter.keys():\n",
    "        temp_content = process_content(twitter['twitter_content']+' '.join(twitter['hashtag_content']))\n",
    "        #temp_content = ' '.join(temp_content)\n",
    "        texts.append(temp_content)\n",
    "    else:\n",
    "        temp_content =process_content(twitter['twitter_content'])\n",
    "        #temp_content = ' '.join(temp_content)\n",
    "        texts.append(temp_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9998\n"
     ]
    }
   ],
   "source": [
    "print len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/DerekWang/anaconda/lib/python2.7/site-packages/gensim/models/ldamodel.py:694: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=998, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(383, u'0.217*\"selfie\" + 0.147*\"co\" + 0.147*\"http\" + 0.115*\"wedding\" + 0.042*\"Baby\" + 0.027*\"Novotel\" + 0.017*\"recipient\" + 0.006*\"Norwest\" + 0.006*\"samptomorrowfund\" + 0.006*\"tomorrowmaker\"'), (581, u'0.107*\"project\" + 0.075*\"http\" + 0.074*\"co\" + 0.059*\"shooting\" + 0.058*\"sm\" + 0.056*\"September\" + 0.029*\"om\" + 0.027*\"att\" + 0.017*\"eller\" + 0.008*\"MishBridges\"'), (184, u'0.411*\"bad\" + 0.237*\"Lol\" + 0.077*\"LA\" + 0.000*\"I\" + 0.000*\"go\" + 0.000*\"want\" + 0.000*\"isdonn\" + 0.000*\"mackaysuzie\" + 0.000*\"Alan\" + 0.000*\"notgavin\"'), (207, u'0.252*\"medium\" + 0.074*\"coverage\" + 0.057*\"cost\" + 0.049*\"traffic\" + 0.048*\"profile\" + 0.045*\"dollar\" + 0.037*\"Birmingham\" + 0.035*\"location\" + 0.026*\"Saturdays\" + 0.019*\"ffs\"'), (447, u'0.147*\"final\" + 0.094*\"eat\" + 0.089*\"http\" + 0.089*\"co\" + 0.084*\"Asian\" + 0.079*\"Appliance\" + 0.061*\"journey\" + 0.028*\"lime\" + 0.018*\"chilli\" + 0.015*\"obsessed\"'), (795, u'0.059*\"co\" + 0.059*\"http\" + 0.056*\"chingchongchinagirl\" + 0.056*\"vivilee\" + 0.056*\"sensationalviv\" + 0.042*\"chinesegirl\" + 0.042*\"chingchong\" + 0.042*\"chinese\" + 0.025*\"Handmade\" + 0.025*\"Inspiration\"'), (715, u'0.000*\"notgavin\" + 0.000*\"AFLGF\" + 0.000*\"Aluminiumilism\" + 0.000*\"mackaysuzie\" + 0.000*\"Alan\" + 0.000*\"Mission\" + 0.000*\"embarrassed\" + 0.000*\"genuinely\" + 0.000*\"GoSwansgf\" + 0.000*\"Christchurch\"'), (169, u'0.156*\"Open\" + 0.126*\"given\" + 0.058*\"sino\" + 0.037*\"Vamos\" + 0.015*\"http\" + 0.015*\"co\" + 0.011*\"Hahahhaa\" + 0.011*\"Ulol\" + 0.011*\"bahala\" + 0.011*\"kaba\"'), (74, u'0.171*\"hahahaha\" + 0.018*\"moloneygeorgia\" + 0.000*\"made\" + 0.000*\"say\" + 0.000*\"promise\" + 0.000*\"AndrewLamingMP\" + 0.000*\"said\" + 0.000*\"never\" + 0.000*\"Lyn\" + 0.000*\"issue\"'), (916, u'0.359*\"so\" + 0.069*\"http\" + 0.068*\"co\" + 0.050*\"tee\" + 0.009*\"Poppo\" + 0.009*\"gruh\" + 0.009*\"PERTH\" + 0.009*\"gbeqfqMjsn\" + 0.000*\"The\" + 0.000*\"z\"'), (964, u'0.224*\"hit\" + 0.110*\"Over\" + 0.077*\"soul\" + 0.042*\"rare\" + 0.037*\"http\" + 0.036*\"co\" + 0.023*\"commitment\" + 0.023*\"id\" + 0.015*\"Bored\" + 0.008*\"Rage\"'), (996, u'0.554*\"lol\" + 0.153*\"young\" + 0.054*\"I\" + 0.008*\"co\" + 0.005*\"PrivateFearless\" + 0.005*\"TheBKGaming\" + 0.000*\"http\" + 0.000*\"VIC\" + 0.000*\"Apartment\" + 0.000*\"Southbank\"'), (13, u'0.208*\"strong\" + 0.086*\"brave\" + 0.073*\"abcnews\" + 0.068*\"hide\" + 0.056*\"arrives\" + 0.030*\"amount\" + 0.009*\"narelleford\" + 0.009*\"xskinn\" + 0.009*\"toogoolawah\" + 0.009*\"weakness\"'), (867, u'0.232*\"tried\" + 0.093*\"rate\" + 0.089*\"Would\" + 0.080*\"option\" + 0.010*\"AthenaKottak\" + 0.010*\"Cruecifly\" + 0.010*\"dimensional\" + 0.010*\"dpletikosa\" + 0.010*\"souwalker\" + 0.000*\"ya\"'), (345, u'0.124*\"Once\" + 0.123*\"living\" + 0.104*\"asleep\" + 0.075*\"crack\" + 0.071*\"clean\" + 0.069*\"Start\" + 0.064*\"I\" + 0.000*\"It\" + 0.000*\"one\" + 0.000*\"http\"'), (304, u'0.147*\"NW\" + 0.077*\"Beyonc\\xe9\" + 0.000*\"Falling\" + 0.000*\"performance\" + 0.000*\"C\" + 0.000*\"today\" + 0.000*\"Barometer\" + 0.000*\"Humidity\" + 0.000*\"Wind\" + 0.000*\"Rain\"'), (549, u'0.269*\"since\" + 0.112*\"I\" + 0.072*\"pancake\" + 0.044*\"http\" + 0.043*\"co\" + 0.043*\"lel\" + 0.022*\"Been\" + 0.008*\"Dome\" + 0.008*\"Midland\" + 0.000*\"awhile\"'), (750, u'0.108*\"distance\" + 0.064*\"America\" + 0.017*\"australialatinamericasymposium\" + 0.017*\"shorten\" + 0.017*\"Latin\" + 0.017*\"UniofAdelaideaustralialatinamericasymposium\" + 0.000*\"Lets\" + 0.000*\"Australia\" + 0.000*\"happytummy\" + 0.000*\"genuinely\"'), (176, u'0.000*\"PLEASE\" + 0.000*\"u\" + 0.000*\"interview\" + 0.000*\"A\" + 0.000*\"I\" + 0.000*\"FOLLOW\" + 0.000*\"CAN\" + 0.000*\"GET\" + 0.000*\"san\" + 0.000*\"dash\"'), (770, u'0.308*\"top\" + 0.122*\"fire\" + 0.052*\"http\" + 0.052*\"co\" + 0.035*\"Event\" + 0.025*\"shelf\" + 0.018*\"scotch\" + 0.007*\"mac\" + 0.007*\"applesandpearsent\" + 0.007*\"Macallan\"')]\n"
     ]
    }
   ],
   "source": [
    "print ldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation  \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "non_words = list(punctuation)\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "\n",
    "\n",
    "def process_content(tweeter_content):\n",
    "    #only need the alphabetic word\n",
    "    formartted_twitter_content = []\n",
    "    tweeter_content =word_tokenizer.tokenize(tweeter_content)\n",
    "    for word in tweeter_content:\n",
    "        if (word.isalpha() != False) and (word not in non_words) and (word not in stop_words):\n",
    "            formartted_twitter_content.append(lemmatizer.lemmatize(word))\n",
    "    return formartted_twitter_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/DerekWang/anaconda/lib/python2.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from collections import OrderedDict\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sort_orderedDict(orderdict):\n",
    "    return OrderedDict(sorted(orderdict.items(), key = lambda x:x[1], reverse = True))\n",
    "\n",
    "positive_tweets =[]\n",
    "negative_tweets =[]\n",
    "neutral_tweets = []\n",
    "compound_tweets = []\n",
    "\n",
    "\n",
    "for twitter in content_data:\n",
    "    if 'hashtag_content' in twitter.keys():\n",
    "        temp_content = process_content(twitter['twitter_content']+' '.join(twitter['hashtag_content']))\n",
    "        temp_content = ' '.join(temp_content)\n",
    "        result = sort_orderedDict(sid.polarity_scores(temp_content))\n",
    "        if result.keys()[0] == 'pos':\n",
    "            positive_tweets.append(temp_content)\n",
    "        elif result.keys()[0] == 'neg':\n",
    "            negative_tweets.append(temp_content)\n",
    "        elif result.keys()[0] == 'neu':\n",
    "            neutral_tweets.append(temp_content)\n",
    "        elif result.keys()[0] == 'compound':\n",
    "            compound_tweets.append(temp_content)\n",
    "    else:\n",
    "        temp_content =process_content(twitter['twitter_content'])\n",
    "        temp_content = ' '.join(temp_content)\n",
    "        result = sort_orderedDict(sid.polarity_scores(temp_content))\n",
    "        if result.keys()[0] == 'pos':\n",
    "            positive_tweets.append(temp_content)\n",
    "        elif result.keys()[0] == 'neg':\n",
    "            negative_tweets.append(temp_content)\n",
    "        elif result.keys()[0] == 'neu':\n",
    "            neutral_tweets.append(temp_content)\n",
    "        elif result.keys()[0] == 'compound':\n",
    "            compound_tweets.append(temp_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438\n",
      "368\n",
      "7877\n",
      "1315\n"
     ]
    }
   ],
   "source": [
    "print len(positive_tweets)\n",
    "print len(negative_tweets)\n",
    "print len(neutral_tweets)\n",
    "print len(compound_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning Brisbane Sunrise noon sunset AEST UTC March Day length sunrise\n"
     ]
    }
   ],
   "source": [
    "print neutral_tweets[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('coordinated_universal_time.n.01.coordinated_universal_time'), Lemma('coordinated_universal_time.n.01.UTC')]\n"
     ]
    }
   ],
   "source": [
    "print lemmas_of_a_word('UTC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject oriented program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from collections import OrderedDict\n",
    "\n",
    "#check the lemmas for a given word\n",
    "def lemmas_of_a_word(word):\n",
    "    lemmas = []\n",
    "    for item in wn.synsets(word):\n",
    "        for lemma in item.lemmas():\n",
    "            lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "#check the matched lemmas for given lemmas\n",
    "def matched_lemma(lemmas, word):\n",
    "    match_lemma =[]\n",
    "    for lemma in lemmas:\n",
    "        if lemma.name().lower() == word.lower() or lemma.name() == word:\n",
    "            match_lemma.append(lemma)\n",
    "    return match_lemma\n",
    "\n",
    "#filter out the primary senses\n",
    "def primary_sense(word):\n",
    "    lemmas_order_dic ={}\n",
    "    for item in matched_lemma(lemmas_of_a_word(word),word):\n",
    "        lemmas_order_dic[item.key()] = item.count()\n",
    "    return lemmas_order_dic\n",
    "\n",
    "#sort the dict to filter the primary sense\n",
    "def sort_orderedDict(orderdict):\n",
    "    return OrderedDict(sorted(orderdict.items(), key = lambda x:x[1], reverse = True))\n",
    "\n",
    "\n",
    "#store the sysnset for the most common sense\n",
    "def store_synset_primarySense(word):\n",
    "    result = {}\n",
    "    check_item = sort_orderedDict(primary_sense(word.lower()))\n",
    "    if len(check_item)==1:\n",
    "        if wn.lemma_from_key(check_item.keys()[0]).synset().pos() == 'n' or wn.lemma_from_key(check_item.keys()[0]).synset().pos() == 'v':\n",
    "                result[word] = wn.lemma_from_key(check_item.keys()[0]).synset()\n",
    "    elif len(check_item)>1:\n",
    "        for index in range(len(check_item.keys())):\n",
    "            if wn.lemma_from_key(check_item.keys()[index]).synset().pos() == 'n' or wn.lemma_from_key(check_item.keys()[index]).synset().pos() == 'v':\n",
    "                result[word] = wn.lemma_from_key(check_item.keys()[index]).synset()\n",
    "                continue\n",
    "            else:\n",
    "                pass\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "#use the lemmatizer defined in the previous workshop\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('trump.n.01.trump'), Lemma('trump.n.01.trump_card'), Lemma('trump.n.02.trump'), Lemma('cornet.n.01.cornet'), Lemma('cornet.n.01.horn'), Lemma('cornet.n.01.trumpet'), Lemma('cornet.n.01.trump'), Lemma('trump.v.01.trump'), Lemma('outdo.v.02.outdo'), Lemma('outdo.v.02.outflank'), Lemma('outdo.v.02.trump'), Lemma('outdo.v.02.best'), Lemma('outdo.v.02.scoop'), Lemma('trump.v.03.trump'), Lemma('trump.v.03.ruff'), Lemma('trump.v.04.trump'), Lemma('trump.v.04.trump_out')]\n"
     ]
    }
   ],
   "source": [
    "print lemmas_of_a_word(\"Trump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on a simple case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between crime and killer: 0.325044754744\n",
      "similarity between crime and murder: 0.121953693758\n",
      "similarity between crime and bomb: 0.402373581011\n",
      "similarity between crime and accident: 0.453937472425\n",
      "similarity between crime and theft: 0.595804068802\n",
      "similarity between crime and robbery: 0.549283046552\n",
      "similarity between crime and sentence(accident happen in melbourne): 0.256422182924\n"
     ]
    }
   ],
   "source": [
    "test_data1 =  ['killer']\n",
    "test_data2 =  ['murder']\n",
    "test_data3 =  ['bomb']\n",
    "test_data4 =  ['accident']\n",
    "test_data5 =  ['theft']\n",
    "test_data6 =  ['robbery']\n",
    "test_data7 =  ['accident','happen','in','melbourne']\n",
    "\n",
    "def cal_happiness(test_data):\n",
    "    twitter_synset =[]\n",
    "    for word in test_data:\n",
    "    #print word\n",
    "        synset = store_synset_primarySense(lemmatize(word))\n",
    "        if (len(synset) >0):\n",
    "            twitter_synset.append(synset)\n",
    "    \n",
    "    wu_palmer_score = 0\n",
    "    wu_palmer_score1 =0\n",
    "    wu_palmer_score2 =0\n",
    "    wu_palmer_score3 =0\n",
    "    wu_palmer_score4 =0\n",
    "    wu_palmer_score5 = 0\n",
    "    wu_palmer_score6 =0\n",
    "    for word in twitter_synset:\n",
    "        wu_palmer_score += wn.wup_similarity(word.values()[0],store_synset_primarySense('crime').values()[0])\n",
    "        wu_palmer_score1 += wn.wup_similarity(word.values()[0], store_synset_primarySense('homicide').values()[0])\n",
    "        wu_palmer_score2 += wn.wup_similarity(word.values()[0], store_synset_primarySense('robbery').values()[0])\n",
    "        wu_palmer_score3 += wn.wup_similarity(word.values()[0], store_synset_primarySense('weapon').values()[0])\n",
    "        wu_palmer_score4 += wn.wup_similarity(word.values()[0], store_synset_primarySense('burglary').values()[0])\n",
    "        wu_palmer_score5 += wn.wup_similarity(word.values()[0], store_synset_primarySense('danger').values()[0])\n",
    "        wu_palmer_score6 += wn.wup_similarity(word.values()[0], store_synset_primarySense('abduction').values()[0])\n",
    "    total = wu_palmer_score+wu_palmer_score1+wu_palmer_score2+wu_palmer_score3+wu_palmer_score4+wu_palmer_score5+wu_palmer_score6\n",
    "    return float(total/7)/float(len(twitter_synset))\n",
    "\n",
    "print 'similarity between crime and killer: ' + str(cal_happiness(test_data1))\n",
    "print 'similarity between crime and murder: ' + str(cal_happiness(test_data2))\n",
    "print 'similarity between crime and bomb: ' + str(cal_happiness(test_data3))\n",
    "print 'similarity between crime and accident: ' + str(cal_happiness(test_data4))\n",
    "print 'similarity between crime and theft: ' + str(cal_happiness(test_data5))\n",
    "print 'similarity between crime and robbery: ' + str(cal_happiness(test_data6))\n",
    "print 'similarity between crime and sentence(accident happen in melbourne): ' + str(cal_happiness(test_data7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.191877560741\n"
     ]
    }
   ],
   "source": [
    "print cal_happiness([\n",
    "    \"Designers\",\n",
    "    \"create\",\n",
    "    \"Ikea\",\n",
    "    \"instruction\",\n",
    "    \"billion\",\n",
    "    \"border\",\n",
    "    \"wall\",\n",
    "    \"http\",\n",
    "    \"co\",\n",
    "    \"jLWthnaQdV\"\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'homicide': Synset('homicide.n.01')}\n"
     ]
    }
   ],
   "source": [
    "print store_synset_primarySense('homicide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
