{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepared the using data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "#save the content data into format like following\n",
    "#[{'twitter_content': 'text,text,text,text', 'hashtag_content': '#whatthewaht'},...]\n",
    "content_data =[]\n",
    "with open('smallTwitter.json') as data_file:\n",
    "    for line in data_file:\n",
    "        content_dic = {}\n",
    "        if line[0] == '{':\n",
    "            line = line[:-1]\n",
    "            if line[-1] == ',':\n",
    "                #get the content\n",
    "                content_dic['twitter_content'] =json.loads(line[:-1]).get('json').get('text')\n",
    "                if len(json.loads(line[:-1]).get('json').get('entities').get('hashtags')) !=0:\n",
    "                    content_dic['hashtag_content'] = json.loads(line[:-1]).get('json').get('entities').get('hashtags')[0].get('text')\n",
    "                content_data.append(content_dic)\n",
    "\n",
    "#create the own word set from the twitter content to expand NLTK wordset\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "\n",
    "words_set = []\n",
    "for twitter in content_data:\n",
    "    words_set +=(word_tokenizer.tokenize(twitter['twitter_content']))\n",
    "words_set = list(set(words_set))\n",
    "\n",
    "from string import punctuation  \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "non_words = list(punctuation)\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "#only need the alphabetic word\n",
    "formartted_twitter_words_set = []\n",
    "for word in words_set:\n",
    "    if (word.isalpha() != False) and (word not in non_words) and (word not in stop_words):\n",
    "        formartted_twitter_words_set.append(lemmatizer.lemmatize(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk_words_set = list(set(nltk.corpus.words.words()))\n",
    "#training whole set\n",
    "training_set = formartted_twitter_words_set+nltk_words_set\n",
    "\n",
    "#split the word in hashtags by using the training set\n",
    "#using backward MaxMatch\n",
    "def find_match_word(hash_content, wordlist):\n",
    "    split_words = []\n",
    "    while len(hash_content) !=0:\n",
    "        #return the index of the matched word\n",
    "        word, index = check_match(hash_content,wordlist)\n",
    "        split_words.append(word)\n",
    "        #remove the matched words from the original tokens\n",
    "        hash_content = hash_content[len(hash_content)*(-1):index]\n",
    "    return split_words\n",
    "\n",
    "#use WordNetLemmatizer to lemmatize the word\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "def check_match(hash_content,wordlist):\n",
    "    count = 0\n",
    "    i = 0\n",
    "    j = len(hash_content)*(-1)\n",
    "    temp = ''\n",
    "    while i>=j:\n",
    "        i -= 1\n",
    "        #save the temporary word and wait for max match\n",
    "        if lemmatizer.lemmatize(hash_content[i:]) in wordlist and i!=j:\n",
    "            temp = hash_content[i:]\n",
    "            count = i\n",
    "        #if already rich the max lenght, return current saved temporary word\n",
    "        elif lemmatizer.lemmatize(hash_content[i:]) in wordlist and i==j:\n",
    "            temp = hash_content[i:]\n",
    "            return temp,i\n",
    "        else:\n",
    "            #base case: if reach the maximum length and the word is not in wordlist\n",
    "            #return current temporary word\n",
    "            if len(temp)>0 and i==j:\n",
    "                return temp,count\n",
    "            #return one single letter if theres no match\n",
    "            elif len(temp) == 0 and i == j:\n",
    "                return hash_content[i:],-1\n",
    "            else:\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the hash tag content \n",
    "It takes a bit long. May need map reduce or HPC to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split the hashtag content now!!\n",
    "for twitter in content_data:\n",
    "    if 'hashtag_content' in twitter.keys():\n",
    "        twitter['hashtag_content'] = find_match_word(twitter['hashtag_content'].lower(),training_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation  \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "non_words = list(punctuation)\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "\n",
    "\n",
    "def process_content(tweeter_content):\n",
    "    #only need the alphabetic word\n",
    "    formartted_twitter_content = []\n",
    "    tweeter_content =word_tokenizer.tokenize(tweeter_content)\n",
    "    for word in tweeter_content:\n",
    "        if (word.isalpha() != False) and (word not in non_words) and (word not in stop_words):\n",
    "            formartted_twitter_content.append(lemmatizer.lemmatize(word))\n",
    "    return formartted_twitter_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from collections import OrderedDict\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sort_orderedDict(orderdict):\n",
    "    return OrderedDict(sorted(orderdict.items(), key = lambda x:x[1], reverse = True))\n",
    "\n",
    "positive_tweets =[]\n",
    "negative_tweets =[]\n",
    "neutral_tweets = []\n",
    "compound_tweets = []\n",
    "\n",
    "\n",
    "for twitter in content_data:\n",
    "    if 'hashtag_content' in twitter.keys():\n",
    "        temp_content = process_content(twitter['twitter_content']+' '.join(twitter['hashtag_content']))\n",
    "        temp_content = ' '.join(temp_content)\n",
    "        result = sort_orderedDict(sid.polarity_scores(temp_content))\n",
    "        if result.keys()[0] == 'pos':\n",
    "            positive_tweets.append(temp_content)\n",
    "        elif result.keys()[0] == 'neg':\n",
    "            negative_tweets.append(temp_content)\n",
    "        elif result.keys()[0] == 'neu':\n",
    "            neutral_tweets.append(temp_content)\n",
    "        elif result.keys()[0] == 'compound':\n",
    "            compound_tweets.append(temp_content)\n",
    "    else:\n",
    "        temp_content =process_content(twitter['twitter_content'])\n",
    "        temp_content = ' '.join(temp_content)\n",
    "        result = sort_orderedDict(sid.polarity_scores(temp_content))\n",
    "        if result.keys()[0] == 'pos':\n",
    "            positive_tweets.append(temp_content)\n",
    "        elif result.keys()[0] == 'neg':\n",
    "            negative_tweets.append(temp_content)\n",
    "        elif result.keys()[0] == 'neu':\n",
    "            neutral_tweets.append(temp_content)\n",
    "        elif result.keys()[0] == 'compound':\n",
    "            compound_tweets.append(temp_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429\n",
      "356\n",
      "7888\n",
      "1325\n"
     ]
    }
   ],
   "source": [
    "print len(positive_tweets)\n",
    "print len(negative_tweets)\n",
    "print len(neutral_tweets)\n",
    "print len(compound_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning Brisbane Sunrise noon sunset AEST UTC March Day length sunrise\n"
     ]
    }
   ],
   "source": [
    "print neutral_tweets[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('coordinated_universal_time.n.01.coordinated_universal_time'), Lemma('coordinated_universal_time.n.01.UTC')]\n"
     ]
    }
   ],
   "source": [
    "print lemmas_of_a_word('UTC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject oriented program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from collections import OrderedDict\n",
    "\n",
    "#check the lemmas for a given word\n",
    "def lemmas_of_a_word(word):\n",
    "    lemmas = []\n",
    "    for item in wn.synsets(word):\n",
    "        for lemma in item.lemmas():\n",
    "            lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "#check the matched lemmas for given lemmas\n",
    "def matched_lemma(lemmas, word):\n",
    "    match_lemma =[]\n",
    "    for lemma in lemmas:\n",
    "        if lemma.name().lower() == word.lower() or lemma.name() == word:\n",
    "            match_lemma.append(lemma)\n",
    "    return match_lemma\n",
    "\n",
    "#filter out the primary senses\n",
    "def primary_sense(word):\n",
    "    lemmas_order_dic ={}\n",
    "    for item in matched_lemma(lemmas_of_a_word(word),word):\n",
    "        lemmas_order_dic[item.key()] = item.count()\n",
    "    return lemmas_order_dic\n",
    "\n",
    "#sort the dict to filter the primary sense\n",
    "def sort_orderedDict(orderdict):\n",
    "    return OrderedDict(sorted(orderdict.items(), key = lambda x:x[1], reverse = True))\n",
    "\n",
    "\n",
    "#store the sysnset for the most common sense\n",
    "def store_synset_primarySense(word):\n",
    "    result = {}\n",
    "    check_item = sort_orderedDict(primary_sense(word.lower()))\n",
    "    if len(check_item)==1:\n",
    "        if wn.lemma_from_key(check_item.keys()[0]).synset().pos() == 'n' or wn.lemma_from_key(check_item.keys()[0]).synset().pos() == 'v':\n",
    "                result[word] = wn.lemma_from_key(check_item.keys()[0]).synset()\n",
    "    elif len(check_item)>1:\n",
    "        for index in range(len(check_item.keys())):\n",
    "            if wn.lemma_from_key(check_item.keys()[index]).synset().pos() == 'n' or wn.lemma_from_key(check_item.keys()[index]).synset().pos() == 'v':\n",
    "                result[word] = wn.lemma_from_key(check_item.keys()[index]).synset()\n",
    "                continue\n",
    "            else:\n",
    "                pass\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "#use the lemmatizer defined in the previous workshop\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on a simple case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between crime and killer: 0.375\n",
      "similarity between crime and murder: 0.125\n",
      "similarity between crime and bomb: 0.470588235294\n",
      "similarity between crime and accident: 0.533333333333\n",
      "similarity between crime and theft: 0.8\n",
      "similarity between crime and robbery: 0.555555555556\n",
      "similarity between crime and sentence(accident happen in melbourne): 0.258823529412\n"
     ]
    }
   ],
   "source": [
    "test_data1 =  ['killer']\n",
    "test_data2 =  ['murder']\n",
    "test_data3 =  ['bomb']\n",
    "test_data4 =  ['accident']\n",
    "test_data5 =  ['theft']\n",
    "test_data6 =  ['robbery']\n",
    "test_data7 =  ['accident','happen','in','melbourne']\n",
    "\n",
    "def cal_happiness(test_data):\n",
    "    twitter_synset =[]\n",
    "    for word in test_data:\n",
    "    #print word\n",
    "        synset = store_synset_primarySense(lemmatize(word))\n",
    "        if (len(synset) >0):\n",
    "            twitter_synset.append(synset)\n",
    "    \n",
    "    wu_palmer_score = 0\n",
    "    for word in twitter_synset:\n",
    "        wu_palmer_score += wn.wup_similarity(word.values()[0],store_synset_primarySense('crime').values()[0])\n",
    "    return float(wu_palmer_score)/float(len(twitter_synset))\n",
    "\n",
    "print 'similarity between crime and killer: ' + str(cal_happiness(test_data1))\n",
    "print 'similarity between crime and murder: ' + str(cal_happiness(test_data2))\n",
    "print 'similarity between crime and bomb: ' + str(cal_happiness(test_data3))\n",
    "print 'similarity between crime and accident: ' + str(cal_happiness(test_data4))\n",
    "print 'similarity between crime and theft: ' + str(cal_happiness(test_data5))\n",
    "print 'similarity between crime and robbery: ' + str(cal_happiness(test_data6))\n",
    "print 'similarity between crime and sentence(accident happen in melbourne): ' + str(cal_happiness(test_data7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
